{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c3f432-4bcf-4b37-96ed-29b40b73e93f",
   "metadata": {},
   "source": [
    "# **Redes Neuronales Profundas Ortogonales (OrthDNNs)**\n",
    ">**Universidad Nacional de Colombia**\n",
    ">\n",
    ">**Álgebra Lineal Numérica - 2025 I**\n",
    ">\n",
    ">*Mateo Sebastian Ortiz Higuera*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f28124c-07a3-4fdb-9fe0-4c5c7d57da49",
   "metadata": {},
   "source": [
    "## **Introducción**\n",
    "\n",
    "En el presente cuaderno introduciremos las redes neuronales profundas ortogonales (OrthDNNs) mostrando la motivación teórica y algunos algoritmos propuestos por S.Li et al [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b296d-58ff-42c0-b4cf-abd511852eb0",
   "metadata": {},
   "source": [
    "### **Entendiendo las redes neuronales**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cccfe679-32b7-4cc5-a76b-258310597b7a",
   "metadata": {},
   "source": [
    "Para entender las Redes Neuronales Profundas Ortogonales primero entendamos las Redes Neuronales Superficiales/Profundas. \n",
    "\n",
    "Matemáticamente, una *red neuronal superficial* es una familia de modelos no lineales parametrizados por unos pesos $\\phi_i,w_{i,j} \\in \\mathbb{R}$ y un bias $b_i\\in \\mathbb{R}$, de la forma\n",
    "\n",
    "$$F(\\mathbf{x}) = \\phi_0 + \\sum_{i=1}^n \\phi_i a\\left[ \\sum_{j=1}^d w_{i,j} x_j + b_i\\right]  $$\n",
    "\n",
    "en donde la red neuronal tiene $n$ neuronas o unidades ocultas, $\\mathbf{x}\\in\\mathbb{R}^{d}$ y $a$ es una función de activación que induce no linealidad. Por ejemplo, supongamos que la red neuronal tiene como entrada tres valores, tiene $3$ unidades ocultas y  predice dos valores, podemos notar $h_i=a(w_{i1}x_1 + w_{i2}x_2 + w_{i3}x_3 + b_i)$ :\n",
    "\n",
    "<img src=\"image01.jpg\" alt=\"red neuronal superficial con \"  style=\"width:250px; margin:auto\"/>\n",
    "\\begin{align}\n",
    "    y_1 &= \\phi_0 + \\phi_1 h_1 + \\phi_2 h_2 + \\phi_3 h_3 \\\\\n",
    "    y_2 &= \\phi_0 + \\phi_1 h_1 + \\phi_2 h_2 + \\phi_3 h_3 \\\\\n",
    "    \\mathbf{y} &= \\Phi_0 + \\Phi_1 h_1 + \\Phi_2 h_2 + \\Phi_3 h_3 \n",
    "\\end{align}\n",
    "\n",
    "en donde $\\Phi_i = \\begin{bmatrix} \\phi_{1,i} \\\\ \\phi_{2,i} \\end{bmatrix}$, $i=0,1,2,3$.\n",
    "\n",
    "Podemos extender esta noción añadiendo más capas, así tenemos una *red neuronal profunda*; básicamente tenemos una composición de redes neuronales superficiales. Nuestros nuevos pesos serán matrices $W_i\\in \\mathbb{R}^{\\ell\\times d}$ y los bias vectores $b_i\\in \\mathbb{R}^{d}$, en donde $i$ es la capa actual, $\\ell$ es el número de neuronas de la capa anterior y $d$ es el número de neuronas de la capa actual. Dicha familia se puede ver de la siguiente forma:\n",
    "\n",
    "$$ F(\\mathbf{x}) = b_k + W_k a\\left[ b_{k-1} + W_{k-1} a\\left[\\dotsb b_2 + W_2 a\\left[b_1 + W_1 \\mathbf{x}  \\right]  \\right] \\dotsb  \\right]$$\n",
    "\n",
    "o recursivamente, definiendo $h_i = \\begin{cases}W_i a\\left[b_{i-1} + W_{i-1}h_{i-1} \\right], \\text{ si }i>2\\\\ W_2 a\\left[b_1 + W_1 \\mathbf{x}\\right], \\text{ si }i=2\\end{cases}$, tenemos\n",
    "\n",
    "$$ F(\\mathbf{x}) = W_k h_k + b_k $$\n",
    "\n",
    "Tomemos el ejemplo anterior y agreguemos dos capas ocultas más, de la forma:\n",
    "\n",
    "<img src=\"image02.jpg\" alt=\"red neuronal superficial con \"  style=\"width:400px; margin:auto\"/>\n",
    "\n",
    "así tenemos $W_4\\in \\mathbb{R}^{2\\times 4}$, $b_4\\in \\mathbb{R}^{2}$, $W_3\\in \\mathbb{R}^{4\\times 3}$, $b_3\\in \\mathbb{R}^{4}$, $W_2\\in \\mathbb{R}^{3\\times 5}$, $b_2\\in \\mathbb{R}^{3}$, $W_1\\in \\mathbb{R}^{3\\times 5}$ y $b_1\\in \\mathbb{R}^{5}$. Y la función sería:\n",
    "\n",
    "$$ \\mathbf{y}= W_4 h_4 + b_4 $$\n",
    "\n",
    "En el aprendizaje automático, optimizamos una función de pérdida $\\mathcal{L}$ para encontrar los valores de $W_i$ y $b_i$ para que el modelo pueda predecir con muestras no vistas anteriormente. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade955de-2823-49c0-acf6-c24e4e4770d6",
   "metadata": {},
   "source": [
    "### **Un pequeño ejemplo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38a379a-6e6d-4162-b073-a537959de622",
   "metadata": {},
   "source": [
    "En el resto del notebook vamos a trabajar principalmente con el siguiente problema: supongamos que queremos crear una red neuronal que quiere clasificar imágenes del dataset MNIST (imágenes de números escritos a mano). En donde cada imágen tiene un tamaño de $28\\times 28$ pixeles, que equivalen a 784 pixeles. Podemos proponer una primer red neuronal profunda que tenga como entrada los 784 pixeles, con dos capas ocultas y una capa final con 10 salidas, que representan las probabilidades para cada una de las clases (los dígitos del 0 al 9).\n",
    "\n",
    "<img src=\"mnist.jpg\" alt=\"Muestra del Dataset de MNIST \"  style=\"width:400px; margin:auto\"/>\n",
    "\n",
    "El grafo computacional referente a esta propuesta sería podría ser algo así:\n",
    "\n",
    "<img src=\"image03.jpg\" alt=\"Grafo computacional con 4 capas\"  style=\"width:400px; margin:auto\"/>\n",
    "\n",
    "En este modelo, la primera capa representa cada pixel de la imagen, las dos capas ocultas extraen información de los pixeles (patrones de las imágenes) y la última funciona para predecir el digito de la imagen. Entrenamos el modelo para ajustar los valores de las matrices de pesos $W_1,W_2 $ y $ W_3$ para que el modelo aprenda a representar y predecir cada dígito.\n",
    "\n",
    "Se puede ver este ejemplo [aquí](https://adamharley.com/nn_vis/mlp/2d.html).\n",
    "\n",
    "\n",
    "Debe notarse que las imágenes de, por ejemplo, el dígito 2, van a encender las mismas neuronas a lo largo de las capas. Ya que los valores de la entrada serán muy similares y así debería ser a lo largo del cómputo del grafo computacional. Esto da sentido las redes neuronales profundas ortogonales, como veremos a continuación; si conseguimos que los valores de las matrices de pesos $W_i$ logren preservar las relaciones espaciales y los patrones detectados a través de las capas, entonces estos contribuyen a mejorar las predicciones de la red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2848fa3-1686-4818-bef9-0c130bf2adb1",
   "metadata": {},
   "source": [
    "### **Redes Neuronales Profundas Ortogonales**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ae002f-4103-4410-8ef4-699673408b53",
   "metadata": {},
   "source": [
    "Cuando nos referimos a las *redes neuronales profundas ortogonales* hablamos de redes neuronales profundas cuyas matrices de pesos $W_i$ son ortogonales. Esta propiedad garantiza que cada transformación lineal entre capas preserva las distancias y ángulos, es decir una isometría.\n",
    "Generalizando lo observado en un ejemplo anterior, mantener isometrías entre capas permite que los datos que pertenecen a una misma clase (por ejemplo, imágenes del dígito '3') permanezcan agrupados a lo largo de la red. Esto significa que las representaciones internas de ejemplos similares se mantendrán \"cerca\" unas de otras, incluso después de múltiples transformaciones entre capas. De manera análoga, las muestras de diferentes clases (como '3' y '8') seguirán estando separadas.\n",
    "\n",
    "Esto es beneficioso para la tarea de clasificación, ya que facilita que el modelo pueda aprender fronteras de decisión claras entre clases. Por el contrario, si las matrices de pesos no son ortogonales, las transformaciones pueden distorsionar el espacio de representación: los puntos cercanos pueden alejarse, y los distantes pueden acercarse. Esto podría llevar a que el modelo confunda clases distintas y degrade su capacidad de generalización, por ejemplo, podría confundir un '3' y clasificarlo como un '8'.\n",
    "\n",
    "Algo que también debemos notar es que al inducir no linealidad, estamos partiendo el hiperplano de muestras. Más adelante observaremos que podemos mantener isometrías locales con un factor de expansión $\\delta$, que mantendrán las distancias en base a dicho factor en las partes lineales de nuestro modelo.\n",
    "\n",
    "<img src=\"image04.jpg\" alt=\"Representación de una transformación de un espacio a otro \" style=\"width:750px; margin:auto\"/>\n",
    "\n",
    "En la imagen anterior podemos ver más claramente el ejemplo, cada punto de distinto color representa una muestra de alguna de las clases (0,1,2,...,9). Algunos estarán más cerca al resto ya que pueden tener algunas formas similares (1 y 7, o 4 y 9). Y al aplicarse la transformación con $W_1$ ortogonal, los valores preservan estas relaciones de cercanía. Siguiendo con estas transformaciones, en la capa final de predicción, vamos a tener que el modelo puede llegar a separar, acotar y predecir cada categoría de mejor forma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07b95bf-0d2d-4c0d-ab17-c35256eb666b",
   "metadata": {},
   "source": [
    "## **Preliminares y algoritmos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fb2d8a-b823-4b0c-ba10-a5f87a5c2de1",
   "metadata": {},
   "source": [
    "Primero carguemos paquetes necesarios y definamos algunos algoritmos necesarios para el desarrollo del cuaderno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f26b3c1-3d36-4b49-8ac8-71e70aea8592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling Statistics [10745b16-79ce-11e8-11f9-7d13ad32a3b2]\n"
     ]
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5976f9ec-6b9c-4a4e-aec7-5dd9cac1f24e",
   "metadata": {},
   "source": [
    "### *Algoritmo de Householder*\n",
    "\n",
    "Utilizaremos el algoritmo de Householder para desarrollar matrices Ortogonales. Los algoritmos son los mismos implementados en clase, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dcf9e54-d32b-4809-88fb-019e6e1fc4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QFA (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function house(x)\n",
    "    n = size(x,1) #(m,n)\n",
    "    s = x[2:n]'x[2:n]\n",
    "    v = [1; x[2:n]]\n",
    "\n",
    "    if s == 0\n",
    "        β = 0\n",
    "    else\n",
    "        mu = sqrt(x[1]^2 + s)\n",
    "        if x[1] <= 0\n",
    "            v[1] = x[1] - mu\n",
    "        else\n",
    "            v[1] = -s /(x[1]+mu)\n",
    "        end\n",
    "        β = 2((v[1])^2)/(s + (v[1])^2)\n",
    "        v=v/v[1]\n",
    "    end\n",
    "    return v, β\n",
    "end\n",
    "\n",
    "function QFA(A)\n",
    "    Q = UniformScaling(1)\n",
    "    m,n = size(A)\n",
    "    for j = 1:n\n",
    "        v,β = house(A[j:m,j])\n",
    "        v1 = zeros(j-1,1)\n",
    "        v = [v1;v]\n",
    "        Qj = UniformScaling(1)-β*v*v'\n",
    "        Q = Q*Qj\n",
    "        A=Qj*A;\n",
    "    end\n",
    "    return Q, A\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcf9628-dda4-46a9-bb93-37430309bec7",
   "metadata": {},
   "source": [
    "### *Ejemplo*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fa008a-df08-45bd-ac2d-0fdc446f3ae1",
   "metadata": {},
   "source": [
    "Veamos un pequeño ejemplo de la intuición descrita en la introducción. Creemos una matriz $A$, con ella generamos una matriz ortogonal $Q$ que serviría como matriz de pesos en la red neuronal. Después definimos dos vectores que podrían ser muestras cercanas de una misma categoría. Finalmente calculemos las distancias entre los vectores antes/después de las transformaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a7958ba-34d3-4367-bdfb-6d72e3bae05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distancia entre los vectores en el espacio original:0.07875658102039006\n",
      "Distancia entre los vectores tras la transformación ortogonal:0.07875658102039024\n",
      "Distancia entre los vectores tras la transformación no ortogonal:0.2017897522729214\n"
     ]
    }
   ],
   "source": [
    "A = rand(5,5)\n",
    "\n",
    "Q,R = QFA(A)\n",
    "x = rand(5)\n",
    "y = x + 0.1*rand(5)\n",
    "\n",
    "println(\"Distancia entre los vectores en el espacio original:\",norm(x - y))\n",
    "println(\"Distancia entre los vectores tras la transformación ortogonal:\",norm(x'*Q - y'*Q))\n",
    "println(\"Distancia entre los vectores tras la transformación no ortogonal:\",norm(x'*A - y'*A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f202474-633c-46b0-981d-009ecea634f3",
   "metadata": {},
   "source": [
    "Se puede observar que la transformación ortogonal preserva la distancia entre los vectores, mientras que la transformación con la matriz no ortogonal no lo hace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b993cca-ff22-4a06-9982-f35e113d8398",
   "metadata": {},
   "source": [
    "## **Formalización matemática**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78c5530-af6b-49f4-a3be-b677c9d418db",
   "metadata": {},
   "source": [
    "Como ya se dijo anteriormente, en este cuaderno trabajaremos con el problema de clasificación de dígitos a mano, el cual es un problema de *Aprendizaje de clasificación-representación* (CRL). En general, en un problema de esta categoría, el modelo debe aprender a representar el espacio de muestras y después aprender la clasificación de los mismos. \n",
    "\n",
    "Formalmente tenemos un espacio de muestras $\\mathcal{Z} = \\mathcal{X}\\times \\mathcal{Y}$, en donde $\\mathcal{X}$ es el espacio de instancias y $\\mathcal{Y}$ el espacio de etiquetas. En donde $x\\in\\mathcal{X}$ es un vector en $\\mathbb{R}^n$ y $y\\in\\mathcal{Y}$ es menor que $|\\mathcal{Y}|$.\n",
    "\n",
    "Podemos denotar el conjunto de entrenamiento de tamaño $m$ como $S_m := \\{s_i=(x_i,y_i) \\}^m_{i=1}$, cuyas muestras son extraídas de manera independiente e idénticamente distribuida de acuerdo a una distrución desconocida $P$. También notamos $S_m^{(x)}=\\{x_i\\}_{i=1}^m$.\n",
    "Dada una función de pérdida $\\mathcal{L}$, el objetivo del aprendizaje es identificar una función $f_{S_m}:\\mathcal{X}\\longrightarrow\\mathcal{Y}$ en un espacio de hipótesis y una función $T:\\mathcal{X}\\longrightarrow \\mathcal{H}$ de representación, la cual extrae las características de la muestra para la tarea de clasificación, que minimicen el riesgo esperado\n",
    "\n",
    "$$R(f,T) = \\mathbb{E}_{z\\sim P}\\left[\\mathcal{L}(f(T(x)),y) \\right]$$\n",
    "\n",
    "en donde todo $z=(x,y)\\in\\mathcal{Z}$ es una muestra independiente y sigue la misma distribución de probabilidad $P$. Como la distribución de probabilidad se desconoce, la cantidad observable que sirve como un sustituto del riesgo esperado $R(f)$ es el riesgo empírico.\n",
    "\n",
    "$$\\hat{R}(f,T) = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(f(T(x_i)),y) $$\n",
    "\n",
    "Así podemos definir el *error de generalización*, el cual es la discrepancia entre el riesgo esperado y el riesgo empírico.\n",
    "\n",
    "$$GE(f_{S_m},T)= |R(f_{S_m},T) -\\hat{R}(f_{S_m},T)|$$.\n",
    "\n",
    "Ahora veamos algunas definiciones y generalizaciones sobre los mapeos isométricos.\n",
    "\n",
    "### **Definición**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086b89f7-edc3-4299-9531-41bffdb18065",
   "metadata": {},
   "source": [
    "# **Referencias**\n",
    "\n",
    "* [1] S. Li, K. Jia, Y. Wen, T. Liu and D. Tao, \"Orthogonal Deep Neural Networks,\" in *IEEE Transactions on Pattern Analysis and Machine Intelligence*, vol. 43, no. 4, pp. 1352-1368, 1 April 2021, doi: 10.1109/TPAMI.2019.2948352.\n",
    "\n",
    "* [2] Simon J.D. Prince, \"Understanding Deep Learning,\" *The MIT Press*, 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c4546-e4b8-4d8a-a490-949c11b0994c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.9",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
